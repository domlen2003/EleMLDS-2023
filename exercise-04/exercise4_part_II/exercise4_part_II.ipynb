{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcb5bab",
   "metadata": {},
   "source": [
    "# EleMLDS - Exercise 4 Part II: Logistic Regression\n",
    "In this exercise, you will implement training & evaluation code for a binary logistic regression model, using both first order and second order methods.\n",
    "\n",
    "Make sure to replace all parts that say\n",
    "```python\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "```\n",
    "\n",
    "Happy coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242b97f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ac145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(name):\n",
    "    data_file = np.load(f\"{name}.npz\")\n",
    "    data = {s: data_file[s] for s in (\"data\", \"labels\")}\n",
    "    data[\"labels\"] = np.where(data[\"labels\"] > 0, 1, 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934a1a6",
   "metadata": {},
   "source": [
    "## Q1: Sigmoid activation and Cross-Entropy Loss\n",
    "**a) (2 points)** First, we will implement the sigmoid activation function and the cross-entropy loss used to train the logistic regression model.\n",
    "\n",
    "Recall that the sigmoid is defined as $\\sigma(a) = \\frac{1}{1 + e^{-a}}$, and the (binary) cross-entropy loss can be written as \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x, \\hat x) = -\\sum_{n=0}^N (x \\ln(\\hat x) + (1 - x) \\ln(1 - \\hat x),\n",
    "$$\n",
    "\n",
    "denoting the predictions with $\\hat x$ and the label with $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3a2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # applies the sigmoid function to the input\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def cross_entropy(x, t):\n",
    "    # computes the binary cross-entropy error\n",
    "    # Input\n",
    "    #  x : array of model outputs (N,) where N is the number of samples (N,) where N is the number of samples\n",
    "    #  t : array of labels, (0 or 1)\n",
    "    # Output\n",
    "    #  cross-entropy error between x and t\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe5d855",
   "metadata": {},
   "source": [
    "**b) (2 points)** Now complete the two functions below.\n",
    "`apply` should compute the posterior probability of the data given the model, and `classify` should output the class prediction for each sample (i.e. either 0 or 1).\n",
    "\n",
    "Assume that the bias is already part of the weight vector $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(w, X):\n",
    "    # applies the model to the dataset and returns the predicted posterior probability\n",
    "    # Input\n",
    "    #  w : model parameters (D,)\n",
    "    #  X : data (N, D) where N is the number of samples and D is the number of features (N, D) where N is the number of samples and D is the number of features\n",
    "    # Output\n",
    "    #  posterior probability for every datapoint (N, )\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def classify(w, X):\n",
    "    # applies the model and classifies datapoints\n",
    "    # Input\n",
    "    #  w : model parameters (D,)\n",
    "    #  X : data (N, D) where N is the number of samples and D is the number of features (N, D) where N is the number of samples and D is the number of features\n",
    "    # Output\n",
    "    #  predicted class label for every datapoint (0 or 1) (N, )\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0435fbf4",
   "metadata": {},
   "source": [
    "The code cell below loads a pretrained model and applies it to one of the test sets. When your implementation is correct, you should get an accuracy above 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d75f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data = np.load(\"check_data.npz\")\n",
    "data = check_data[\"data\"]\n",
    "labels = check_data[\"labels\"]\n",
    "weight = check_data[\"weight\"]\n",
    "\n",
    "data = np.hstack((np.ones((data.shape[0], 1)), data))\n",
    "\n",
    "test_accuracy = np.mean(classify(weight, data) == labels)\n",
    "\n",
    "print(f\"Accuracy of the classify function : {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74ff8c",
   "metadata": {},
   "source": [
    "In the next cell, we provide some evaluation and visualization utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contour(fun):\n",
    "    # make a regular grid over the whole plot\n",
    "    x, y = np.linspace(*plt.xlim(), 200), np.linspace(*plt.ylim(), 200)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    # evaluate function on grid\n",
    "    res = fun(grid).reshape(*xx.shape)\n",
    "\n",
    "    # plot contour lines (=decision boundary) and filled contours over whole plot\n",
    "    plt.contourf(\n",
    "        xx, yy, res, levels=[-np.inf, 0.5, np.inf], colors=[\"yellow\", \"blue\"], alpha=0.2\n",
    "    )\n",
    "    plt.contour(xx, yy, res, levels=[0.5], colors=\"k\")\n",
    "    # print(res.min(), res.max(), (res**2).sum())\n",
    "\n",
    "\n",
    "def plot_(data, labels, params=None, basis_fun=None):\n",
    "    # Plot the data points and the decision line\n",
    "    plt.subplot()\n",
    "\n",
    "    class1, class2 = data[labels > 0.5], data[labels < 0.5]\n",
    "    plt.scatter(*class1.T, c=\"blue\", marker=\"x\")\n",
    "    plt.scatter(*class2.T, c=\"orange\", marker=\"o\")\n",
    "\n",
    "    if params is not None:\n",
    "        # Quick and hacky way to fix the y-axis limits\n",
    "        plt.ylim(plt.ylim())\n",
    "\n",
    "        if basis_fun:\n",
    "            # evaluate decision function on grid and plot contours\n",
    "            plot_contour(lambda grid: apply(params, basis_fun(grid)))\n",
    "        else:\n",
    "            w, b = params[1:], params[0]\n",
    "            xmax = data[:, 0].max(0)\n",
    "            xmin = data[:, 0].min(0)\n",
    "            # just plot a line\n",
    "            y = lambda x: -(w[0] * x + b) / w[1]\n",
    "            plt.plot([xmin, xmax], [y(xmin), y(xmax)], c=\"k\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def eval_logreg(w, data, labels, phi=None, basis_fun=None):\n",
    "    if phi is None:\n",
    "        phi = lambda x: x\n",
    "\n",
    "    pred = classify(w, phi(data))\n",
    "    acc = np.mean(pred == labels)\n",
    "    print(f\"Accuracy: {acc:.5f}\")\n",
    "\n",
    "    plot_(data, labels, params=w, basis_fun=basis_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee4bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = load_data(\"lc_train\")\n",
    "train_data, train_labels = data_[\"data\"], data_[\"labels\"]\n",
    "data_ = load_data(\"lc_test\")\n",
    "test_data, test_labels = data_[\"data\"], data_[\"labels\"]\n",
    "\n",
    "outlier_train_data = np.append(train_data, [[1.5, -0.4], [1.45, -0.35]], axis=0)\n",
    "outlier_train_labels = np.append(train_labels, [[0], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396c4737",
   "metadata": {},
   "source": [
    "## Q2: Optimization methods\n",
    "**a) (2 points)** In the second part of this exercise, you will implement the training procedure.\n",
    "First, implement the update rule of gradient descent in the function `first_order_update`.\n",
    "Recall that gradient descent updates the weights via\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{(\\tau+1)} = \\mathbf{w}^{(\\tau)} - \\eta \\nabla E(w)\n",
    "$$\n",
    "\n",
    "and the gradient of the logistic regression model (sigmoid activation + cross-entropy error) is\n",
    "\n",
    "$$\n",
    "\\nabla E(\\mathbf{w}) = \\sum_{n=1}^N (y_n - t_n)\\phi_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62178787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_order_update(w, X, t, eta):\n",
    "    # computes the gradient descent update for the given parameters\n",
    "    # Input\n",
    "    #  w : current model parameters (D,)\n",
    "    #  X : data (N, D) where N is the number of samples and D is the number of features (N, D) where N is the number of samples and D is the number of features\n",
    "    #  t : labels (N,)\n",
    "    #  eta : step size of gradient descent (scalar)\n",
    "    # Output\n",
    "    #  new model parameters (D,)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e5eb5",
   "metadata": {},
   "source": [
    "We have provided a training loop below.\n",
    "Have a look at it to see how we utilize the functions you wrote so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136ff77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, labels, err_fun, iters, update_fun, err_thresh=1e-3):\n",
    "    # assume data is already transformed with basis function\n",
    "    w = np.zeros(data.shape[1])\n",
    "\n",
    "    errors, norms = [], []\n",
    "    for _ in range(iters):\n",
    "        result = apply(w, data)\n",
    "        err = err_fun(result, labels)\n",
    "        errors.append(err)\n",
    "        norms.append(np.sum(w**2))\n",
    "        if err < err_thresh:\n",
    "            break\n",
    "        w = update_fun(w, data, labels)\n",
    "\n",
    "    l1 = plt.plot(errors)\n",
    "    plt.title(\"Error and weight norm over iterations\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.twinx()\n",
    "    plt.ylabel(\"Weight norm\")\n",
    "    l2 = plt.plot(norms, c=\"orange\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.legend(l1 + l2, [\"error\", \"norm\"])\n",
    "    plt.show()\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15161a10",
   "metadata": {},
   "source": [
    "Run the code below to test your implementation; what accuracy do you achieve? What do you notice about the model weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6c4241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias(X):\n",
    "    N = len(X)\n",
    "    return np.concatenate([np.ones((N, 1)), X], axis=-1)\n",
    "\n",
    "\n",
    "eta = 0.1\n",
    "iters = 1000\n",
    "phi = add_bias\n",
    "\n",
    "grad_descent_update = partial(first_order_update, eta=eta)\n",
    "w = train(\n",
    "    phi(outlier_train_data),\n",
    "    outlier_train_labels,\n",
    "    cross_entropy,\n",
    "    iters,\n",
    "    grad_descent_update,\n",
    ")\n",
    "eval_logreg(w, outlier_train_data, outlier_train_labels, phi)\n",
    "eval_logreg(w, test_data, test_labels, phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8759a04",
   "metadata": {},
   "source": [
    "**b) (4 points)** Now complete the cell below to implement the IRLS update equations.\n",
    "\n",
    "**Hint:** the matrix $R$ is not always invertible. We can instead use the following trick: finding $A^{-1}b$ is equivalent to solving $Ax = b$ for $x$; finding a good approximation for $x$ is relatively easy, while inverting the matrix is impossible. With numpy, we would use `np.linalg.lstsq(A, b)` to get the least-squares approximation to $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047c1d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_order_update(w, X, t):\n",
    "    # compute the second-order (IRLS) update for the given parameters\n",
    "    # Input\n",
    "    #  w : current model parameters (D,)\n",
    "    #  X : data (N, D) where N is the number of samples and D is the number of features (N, D) where N is the number of samples and D is the number of features\n",
    "    #  t : labels (N,)\n",
    "    # Output\n",
    "    #  new model parameters (D,)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ee0f5",
   "metadata": {},
   "source": [
    "Now train and evaluate a logistic regression model using this function.\n",
    "Notice that the second-order updater needs fewer iterations than the first-order method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb2c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_second_order = 10\n",
    "w = train(\n",
    "    phi(train_data), train_labels, cross_entropy, iter_second_order, second_order_update\n",
    ")\n",
    "eval_logreg(w, outlier_train_data, outlier_train_labels, phi)\n",
    "eval_logreg(w, test_data, test_labels, phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd7eeb",
   "metadata": {},
   "source": [
    "### Test different datasets\n",
    "\n",
    "Now, we will evaluate the weight update algorithm on various datasets to examine its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba064ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly(x, d):\n",
    "    feature = np.stack(\n",
    "        [x[:, 0] ** i * x[:, 1] ** j for i in range(d + 1) for j in range(d + 1 - i)],\n",
    "        axis=-1,\n",
    "    )\n",
    "    return feature\n",
    "\n",
    "\n",
    "def play_around(phi, update_fun, dataset, iters):\n",
    "    data_ = load_data(f\"{dataset}_train\")\n",
    "    train_data, train_labels = data_[\"data\"], data_[\"labels\"]\n",
    "    data_ = load_data(f\"{dataset}_test\")\n",
    "    test_data, test_labels = data_[\"data\"], data_[\"labels\"]\n",
    "\n",
    "    w = train(phi(train_data), train_labels, cross_entropy, iters, update_fun, 2.5)\n",
    "    eval_logreg(w, train_data, train_labels, phi, phi)\n",
    "    eval_logreg(w, test_data, test_labels, phi, phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e4b6cf",
   "metadata": {},
   "source": [
    "To modify the degree of the polynomial kernel and select different update algorithms, you can adjust the values of the following variables:\n",
    "\n",
    "- `d`: Represents the degree of the polynomial kernel. It should be set to a positive integer.\n",
    "- `eta`: The learning rate (for the first-order udpate)\n",
    "\n",
    "Test out different configurations on different datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b0d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 1000\n",
    "eta = 0.1\n",
    "d = 2\n",
    "phi = partial(poly, d=d)\n",
    "GD = partial(first_order_update, eta=eta)\n",
    "IRLS = partial(second_order_update)\n",
    "\n",
    "# possible values for dataset are:\n",
    "# lc, circles, CC, XOR\n",
    "play_around(\n",
    "    phi=phi,\n",
    "    update_fun=GD,\n",
    "    dataset=\"circles\",\n",
    "    iters=iters,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
