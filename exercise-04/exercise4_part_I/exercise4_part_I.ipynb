{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd037773",
   "metadata": {},
   "source": [
    "# EleMLDS - Exercise 4 Part I: Linear Regression\n",
    "In this exercise, you will implement linear regression using the least-squares formulation as well as ridge regression for improved robustness against overfitting.\n",
    "\n",
    "Make sure to replace all parts that say\n",
    "```python\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "```\n",
    "\n",
    "Happy coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa92ac44",
   "metadata": {},
   "source": [
    "# Q1: Implement least squares regression\n",
    "In this first part of the exercise, you implement the least-squares solution to linear regression.\n",
    "\n",
    "**Hint:** this is very similar to training a least-squares classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558280d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T16:59:36.038288Z",
     "start_time": "2024-01-10T16:59:34.737093Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "198bcdc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T16:59:37.938007Z",
     "start_time": "2024-01-10T16:59:37.924983Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(name):\n",
    "    data = np.load(f\"{name}.npz\")\n",
    "    return tuple(data[f] for f in data.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c1cc491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T16:59:44.353499Z",
     "start_time": "2024-01-10T16:59:44.346478Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_(data, label, params=None, basis_fun=None):\n",
    "    # Plot the data points and the regression function\n",
    "    plt.subplot()\n",
    "\n",
    "    plt.scatter(data.T, label.T, c=\"blue\", marker=\"x\")\n",
    "\n",
    "    if params:\n",
    "        xmin, xmax = plt.xlim()\n",
    "        x = np.linspace(xmin, xmax, 200)[..., None]\n",
    "        fx = linreg(*params, x, basis_fun)\n",
    "        plt.plot(x, fx, c=\"orange\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35003b04",
   "metadata": {},
   "source": [
    "**a) (3 points)** Implement the `least_squares` function that computes the least-squares solution given some labelled data.\n",
    "Afterwards, complete the `linreg` function; this function should apply a given linear regression model on some data.\n",
    "It optionally transforms the data using a basis function, if provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2075a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(data, label):\n",
    "    # computes the parameters of a least-squares regression model\n",
    "    # Input\n",
    "    #  data  : NxD array of data samples\n",
    "    #  label : Nx1 array of (continuous) targets\n",
    "    # Output\n",
    "    #  weight : the D-dim weight vector\n",
    "    #  bias   : the scalar bias term\n",
    "\n",
    "    weight =  np.dot(np.linalg.pinv(data), label)\n",
    "    bias = weight[0]\n",
    "    \n",
    "    return weight, bias\n",
    "\n",
    "\n",
    "def linreg(weight, bias, data, basis_fun=None):\n",
    "    # Applies a linear regression model to the given data, optionally with basis function\n",
    "    # Input\n",
    "    #  weight : the D-dim weight vector\n",
    "    #  bias   : the scalar bias term\n",
    "    #  data   : NxD array of data samples\n",
    "    #  basis_fun : maps an array of samples to an array of feature vectors\n",
    "    # Output\n",
    "    #  predicted label: Nx1 array\n",
    "    #  Be aware that the output.shape should be (N, 1) instead of (N, )!\n",
    "    pred_lable = []\n",
    "    if basis_fun is not None:\n",
    "        data = basis_fun(data)\n",
    "    for i in range(np.shape(data)[0]):\n",
    "        pred_lable = np.append(pred_lable,np.dot(np.transpose(weight), data[i]))\n",
    "    return np.transpose(np.asmatrix(pred_lable))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f6728",
   "metadata": {},
   "source": [
    "**b) (2 points)** We will need some evaluation measures to judge the performance of our models.\n",
    "Implement the mean-squared error (MSE) and root mean square (RMS) functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec87db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(pred, gt):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def rms(pred, gt):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87472a3",
   "metadata": {},
   "source": [
    "Let's apply your implementation on some data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a69b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"sine\"\n",
    "train_data, train_label = load_data(f\"regression_{dataset_name}_train\")\n",
    "test_data, test_label = load_data(f\"regression_{dataset_name}_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f77a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = least_squares(train_data, train_label)\n",
    "plot_(train_data, train_label, params)\n",
    "plot_(test_data, test_label, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd5a5ff",
   "metadata": {},
   "source": [
    "As the example shows, a purely linear model is on its own not able to model complex behavior.\n",
    "However, we can again apply non-linear basis functions to the data.\n",
    "Let's implement some basis function and see what effects we can observe.\n",
    "\n",
    "**c) (3 points)** Implement a polynomial basis function below.\n",
    "Since we work on 1D data here, we can use a simplified formulation:\n",
    "\n",
    "$$\n",
    "\\varphi_d(x) = (1, x, x^2, \\ldots, x^d)^\\mathsf{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2702b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly(x, d):\n",
    "    # Input\n",
    "    #  x : NxD array of data samples (In this case D=1 since input is 1-dimensional)\n",
    "    #  d : degree of the polynomial\n",
    "    # Output\n",
    "    #  feature : Nxd array of transformed features\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86598049",
   "metadata": {},
   "source": [
    "We will now test how well they perform, and see which hyperparameter (i.e. the degree of the polynomial/the maximum frequency of the sinusoid) performs the best.\n",
    "We also plot the norm of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f065ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_basis_function(solver, fun, max_degree):\n",
    "    error_train, error_test, weight_norm = [], [], []\n",
    "    for d in range(1, max_degree):\n",
    "        phi = partial(fun, d=d)\n",
    "        params = solver(phi(train_data), train_label)\n",
    "\n",
    "        # uncomment to visualize the regression results for all degrees\n",
    "        plot_(train_data, train_label, params, phi)\n",
    "        plot_(test_data, test_label, params, phi)\n",
    "\n",
    "        train_pred = linreg(*params, train_data, phi)\n",
    "        test_pred = linreg(*params, test_data, phi)\n",
    "\n",
    "        error_train.append(rms(train_pred, train_label))\n",
    "        error_test.append(rms(test_pred, test_label))\n",
    "        weight_norm.append((params[0] ** 2).sum() + params[1] ** 2)\n",
    "    plt.title(fun.__name__)\n",
    "    ln1 = plt.plot(error_train, label=\"Train error\")\n",
    "    ln2 = plt.plot(error_test, label=\"Test error\")\n",
    "    plt.xlabel(\"Degree\"), plt.ylabel(\"RMS\")\n",
    "    ax2 = plt.twinx()\n",
    "    plt.ylabel(\"Norm of w\")\n",
    "    ln3 = plt.plot(weight_norm, label=\"weight norm\", c=\"green\")\n",
    "    plt.legend(ln1 + ln2 + ln3, [\"Train error\", \"Test error\", \"Weight norm\"])\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Best test error: {min(error_test)}\")\n",
    "\n",
    "\n",
    "evaluate_basis_function(least_squares, poly, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05127cb5",
   "metadata": {},
   "source": [
    "# Q2: Implement regularized least-squares\n",
    "While the results from fitting a regression function with higher-order polynomials look promising, we observed severe overfitting due to the small size of the training set and the large capacity of the model.\n",
    "We will now implement regularization in order to avoid this behavior.\n",
    "\n",
    "**a) (2 points)** Complete the function below to compute the ridge regression solution (i.e. least-squares solution with squared penalty on the norm of the weight vector).\n",
    "Keep in mind that we do not want to penalize the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff91118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_least_squares(data, label, lam):\n",
    "    # Input\n",
    "    #  data  : NxD array of data samples\n",
    "    #  label : Nx1 array of (continuous) targets\n",
    "    #  lam   : lambda, the ridge parameter/regularization coefficient\n",
    "    # Output\n",
    "    #  weight : the D-dim weight vector\n",
    "    #  bias   : the scalar bias term\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return weight, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a93593a",
   "metadata": {},
   "source": [
    "Let's repeat the experiment from above with the regularized least-squares solver.\n",
    "What do you observe now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e357bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lam in (0.001, 0.01, 0.1):\n",
    "    print(f\"##### lambda = {lam} #####\")\n",
    "    solver = partial(regularized_least_squares, lam=lam)\n",
    "    evaluate_basis_function(solver, poly, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
